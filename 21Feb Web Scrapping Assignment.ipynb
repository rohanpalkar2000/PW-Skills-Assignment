{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4854be86-7cf9-4c9e-8c91-525c1c085a43",
   "metadata": {},
   "source": [
    "# Question 1: What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bed3c7-9866-4621-90c3-1280c3ed116e",
   "metadata": {},
   "source": [
    "Web scraping is a term used to describe the use of a program or algorithm to extract and process large amounts of data from the web. Whether you are a data scientist, engineer, or anybody who analyzes large amounts of datasets, the ability to scrape data from the web is a useful skill to have. Let's say you find data from the web, and there is no direct way to download it, web scraping using Python is a skill you can use to extract the data into a useful form that can be imported.\n",
    " Web scrapping is used for:\n",
    " * Data extraction from the web using Python's Beautiful Soup module\n",
    " * Data manipulation and cleaning using Python's Pandas library\n",
    " * Data visualization using Python's Matplotlib library\n",
    " \n",
    " Web scraping is used in a variety of digital businesses that rely on data harvesting. Legitimate use cases include: Search engine bots crawling a site, analyzing its content and then ranking it. Price comparison sites deploying bots to auto-fetch prices and product descriptions for allied seller websites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36839be3-2f4c-443d-84c2-63660d1a8c48",
   "metadata": {},
   "source": [
    "# Question 2: What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997adcc4-fde2-463f-826f-181fb4e58247",
   "metadata": {},
   "source": [
    "### 1. BeautifulSoup\n",
    "BeautifulSoup offers a Pythonic interface and automated encoding conversions, making it easier to work with website data. provides various Pythonic idioms and methods for browsing, exploring, and altering a parse tree. Furthermore, you can set up BeautifulSoup to scan a whole parsed page, identify all repetitions of the data you need (for instance, find all links in a document), or automatically detect encodings like special characters with only a few lines of code.\n",
    "\n",
    "### 2. Scrapy\n",
    "Scrapy is one of the most popular Python web scraping libraries. Scrapy is a web crawling and screen scraping library to quickly and efficiently crawl websites and extract structured data from their pages. You can use Scrapy as more than just a library, i.e., you can use it for various tasks, including monitoring, automated testing, and data mining. Scrapy uses an auto-throttling method to alter crawling speed automatically. It also provides developer accessibility.\n",
    "\n",
    "### 3. Selenium\n",
    "Selenium is a free and open-source web driver that enables you to automate tasks like logging onto social media sites. It works efficiently on JavaScript-rendered web pages, which is unusual for other Python libraries. You must first create functional test cases using the Selenium web driver before you can begin working on Selenium with Python. The Selenium library works well with any browser, such as Firefox, Chrome, IE, etc., for testing. The most common approach to integrating Selenium with Python is through APIs, which help create functional or acceptance test cases with the Selenium web driver. \n",
    "\n",
    "### 4. Requests\n",
    "Requests is another popular Python library that makes it easier to generate multiple HTTP requests. This is extremely helpful for web scraping since the primary step in any web scraping process is to submit HTTP requests to the website's server to extract the data displayed on the desired web page. The first stage of the web scraping process benefits from using the Requests library.\n",
    "\n",
    "### 5. Urllib3\n",
    "Urllib3 is a popular Python web scraping library that can quickly extract data from HTML documents or URLs, similar to the requests library in Python. You can retrieve URLs with the help of the Python package urllib request. The URL open method offers a user interface that is quite simple, and this is capable of retrieving URLs via several protocols.\n",
    "\n",
    "### 6. Lxml\n",
    "LXML is the most feature-rich and user-friendly Python library for parsing XML and HTML. It is a robust Pythonic binding for the libxml2 and libxslt libraries. The ElementTree API allows convenient and safe access to these web scraping libraries. It enriches the ElementTree API by adding support for XPath, RelaxNG, XML Schema, XSLT, C14N, and many other languages. It functions well when you try to scrape massive databases. Web scraping frequently uses requests and lxml together.\n",
    "\n",
    "### 7. MechanicalSoup\n",
    "MechanicalSoup is one of Python's latest web scraping libraries that allows automated website interaction. Built on the powerful and popular Python libraries Requests (for HTTP sessions) and BeautifulSoup (for document navigation), MechanicalSoup offers a similar API to these two powerful web scraping libraries. This web scraper can follow redirects, send cookies automatically, follow links, and submit forms.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1585e943-2e39-4542-90a0-2bce17459a62",
   "metadata": {},
   "source": [
    "# Question 3: What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e3da05-834a-4972-be20-0c3e38e0b6f5",
   "metadata": {},
   "source": [
    "Beautiful Soup is a Python library for getting data out of HTML, XML, and other markup languages. Say you’ve found some webpages that display data relevant to your research, such as date or address information, but that do not provide any way of downloading the data directly. Beautiful Soup helps you pull particular content from a webpage, remove the HTML markup, and save the information. It is a tool for web scraping that helps you clean up and parse the documents you have pulled down from the web.\n",
    "\n",
    "### library :\n",
    "pip install beautifulsoup4\n",
    "\n",
    "### Use of Beautiful Soup :-\n",
    "The Beautiful Soup library helps with isolating titles and links from webpages. It can extract all of the text from ​HTML tags, and alter the HTML ​in the document with which we’re working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b13a44-02ce-40d6-b0c8-9edb595c1775",
   "metadata": {},
   "source": [
    "# Question 4: Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3a95c4-c1c6-4dbc-96f1-165c9e5ccc80",
   "metadata": {},
   "source": [
    "Flask is a lightweight framework to build websites. We'll use this to parse our collected data and display it as HTML in a new HTML file. The requests module allows us to send http requests to the website we want to scrape. The first line imports the Flask class and the render_template method from the flask library.\n",
    "\n",
    "Web scraping is a term used for the process of extracting HTML/XML data from websites. Once extracted, it can be parsed into a different HTML file or saved locally in text/spreadsheet documents.\n",
    "\n",
    "It helps to be a generalist when you’re just starting out. Learn everything, you never know when you’ll need it! You can always settle and specialize in one area eventually, when you’re well aware of the options you have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c748df9d-830c-4132-b919-ece2af96738e",
   "metadata": {},
   "source": [
    "# Question 5: Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee6ae41-7a36-454f-8b3b-0cce6105b960",
   "metadata": {},
   "source": [
    "### Amazon S3\n",
    "Addition to the AWS services list is Amazon S3, which is an object storage AWS service, which is highly scalable. It mainly helps users to access any quantity of data from anywhere. Here, data is stored in ‘storage classes’ to reduce costs without any extra investment and manage it comfortably. \n",
    "\n",
    "### AWS Aurora\n",
    "AWS RDS AWS Service  Amazon Aurora is the next addition to this list of top AWS services in demand. Why? It is a MySQL and PostgreSQL compatible relational database with high performance. Believe it or not, it is five times faster than standard MySQL databases. And it allows for automating crucial tasks such as hardware provisioning, database setup and backups, and patching.\n",
    "\n",
    "### Amazon DynamoDB\n",
    "DynamoDB AWS Service DynamoDB is a promising addition to this list of AWS services. DynamoDB is a fully managed and serverless NoSQL database AWS service. And it is a fast and flexible database system that provides innovative opportunities to developers at low costs. \n",
    "\n",
    "### Amazon RDS\n",
    "AWS RDS AWS Service Amazon RDS would be the next entry in this discussion on AWS services. Amazon RDS is the managed Relational Database AWS Service (RDS) for MySQL, PostgreSQL, Oracle, SQL Server, and MariaDB. It allows the setup, operation, and scale of a relational database in the cloud quickly.\n",
    "\n",
    "### Amazon Lambda\n",
    "AWS Lambda AWS ServiceAWS Lambda is also a promising addition to the list of AWS services. Amazon Lambda is a serverless and event-driven computing AWS service. It helps to run codes automatically without worrying about servers and clusters.\n",
    "\n",
    "### Amazon VPC\n",
    "AWS VPC AWS Service Amazon VPC is the Virtual Private Cloud, which is an isolated cloud resource. It controls the virtual networking environment, such as resource placement, connectivity, and security. And it allows you to build and manage compatible VPC networks across cloud AWS resources and on-premise resources.\n",
    "\n",
    "### Amazon CloudFront\n",
    "Amazon CloudFront AWS Service Amazon CloudFront is another credible mention in the list of renowned Amazon Web Services. This AWS service delivers content globally, which offers high performance and security. Mainly, it delivers data with high speed and low latency. \n",
    "\n",
    "### AWS Elastic Beanstalk\n",
    "AWS Elastic Beanstalk AWS Service  This AWS service supports running and managing web applications. Elastic Beanstalk allows for the easy deployment of applications from capacity provisioning, load balancing, and auto-scaling to application health monitoring. With its auto-scaling properties, this service simplifies demands in scaling to adjust to the needs of the business. \n",
    "\n",
    "### Amazon SNS\n",
    "AWS SNS AWS Service  It is the Amazon Simple Notification Service (SNS). It is a messaging service between Application to Application (A2P) and Application to Person (A2Person). Here, A2P helps many-to-many messaging between distributed systems, microservices, and event-driven serverless applications. And, A2P supports applications to send messages to many users via mail, SMS, etc. For instance, you can send up to ten messages in a single API request.\n",
    "\n",
    "### Amazon EBS\n",
    " AWS EBS AWS Service Amazon Elastic Block Store (EBS) is the block storage service. It supports scaling high-performance workloads such as SAP, Oracle, and Microsoft products. And it provides better protection against failures up to 99.999%. It helps to resize clusters for big data analytics engines such as Hadoop and Spark."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
